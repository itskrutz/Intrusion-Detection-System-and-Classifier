# -*- coding: utf-8 -*-
"""Copy of Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XFZ2z7m7rbso6jfifkRW8tD2_5TvDuJt
"""

import numpy as np
from sklearn import datasets
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive', force_remount = True)

df = pd.read_csv('/content/drive/MyDrive/DataEBPF.csv')

df.info()

df.attack_cat.value_counts()
df.label.value_counts()

import pandas as pd
import torch
from sklearn.preprocessing import StandardScaler, LabelEncoder
from torch.utils.data import TensorDataset

#numerical_columns = ['dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'sinpkt', 'dinpkt', 'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'is_sm_ips_ports', 'label']  # List your numerical column names here
#label_columns = ['proto', 'service', 'attack_cat']

#numerical columns (scaling)
#scaler = StandardScaler()
#df[numerical_columns] = scaler.fit_transform(df[numerical_columns])

#label columns (encoding)
#label_encoders = {}
#for col in label_columns:
#    label_encoders[col] = LabelEncoder()
#    df[col] = label_encoders[col].fit_transform(df[col])


#numerical_data = torch.tensor(df[numerical_columns].values, dtype=torch.float32)

#label_tensors = [torch.tensor(df[col].values, dtype=torch.long) for col in label_columns]

#labels_combined = torch.cat(label_tensors, dim=0)
#labels_combined = labels_combined.view(-1, 3)

#dataset = TensorDataset(numerical_data, labels_combined)
df['label']

import pandas as pd
import torch
from sklearn.preprocessing import StandardScaler, LabelEncoder
from torch.utils.data import TensorDataset

numerical_columns = ['dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'sinpkt', 'dinpkt',  'smean', 'dmean']  # List your numerical column names here
label_columns = ['proto', 'attack_cat']

#numerical columns (scaling)
scaler = StandardScaler()
df[numerical_columns] = scaler.fit_transform(df[numerical_columns])

#label columns (encoding)
label_encoders = {}
for col in label_columns:
    label_encoders[col] = LabelEncoder()
    df[col] = label_encoders[col].fit_transform(df[col])

x=['dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'sinpkt', 'dinpkt', 'smean', 'dmean', 'attack_cat','proto']
y=['label']
print(df['label'])
#X_value=df[x]
#Y_value=df[y]

#print(df[numerical_columns])
#print(df[label_columns])
#print(df[numerical_columns].dtypes)

#X_data = torch.tensor(X_value.values, dtype=torch.float32)

#Y_data= torch.tensor(Y_value.values, dtype=torch.long)
#Y_data=Y_data.view(-1,1)

#X_data.shape
#Y_data.shape
#Y_data.shape
#labels_combined = torch.cat(label_tensors, dim=0)
#labels_combined = labels_combined.view(-1, 3)

numerical_data = torch.tensor(df[x].values, dtype=torch.float32)
label_tensors = torch.tensor(df[y].values, dtype=torch.long)

#labels_combined = torch.cat(label_tensors, dim=0)
#labels_combined = labels_combined.view(-1, 3)

datasett = TensorDataset(numerical_data, label_tensors)

print(label_tensors)

print(numerical_data.shape)
print(label_tensors.shape)

import torch.nn.functional as F

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset, random_split

class LSTMClassifier(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes):
        super(LSTMClassifier, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        out, _ = self.lstm(x)
        print(out.shape)
        out = self.fc(out[:, -1, :])
        return out

# Assuming you already have your data loaded into numerical_data and label_tensors

# Creating a dataset
dataset = TensorDataset(numerical_data, label_tensors)

# Define batch size and split sizes
batch_size = 64
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size

# Splitting the dataset into train and validation sets
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

# Creating data loaders
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# Model, criterion, and optimizer initialization
model = LSTMClassifier(input_size=16, hidden_size=128, num_layers=2, num_classes=1)  # Adjust num_classes accordingly
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.02)



"""## BURRRR"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from sklearn.preprocessing import StandardScaler, LabelEncoder
from torch.utils.data import TensorDataset, DataLoader, random_split

# Define the Focal Loss class
class FocalLoss(nn.Module):
    def __init__(self, gamma=2, alpha=None, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.gamma = gamma
        self.alpha = alpha
        self.reduction = reduction

    def forward(self, input, target):
        ce_loss = F.cross_entropy(input, target, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = (1 - pt) ** self.gamma * ce_loss

        if self.alpha is not None:
            focal_loss = self.alpha * focal_loss

        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

# Mount Google Drive
drive.mount('/content/drive', force_remount=True)

# Read the CSV file
df = pd.read_csv('/content/drive/MyDrive/DataEBPF.csv')

# Define numerical and label columns
numerical_columns = ['dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'sinpkt', 'dinpkt', 'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'is_sm_ips_ports']
label_columns = ['proto', 'service', 'attack_cat']

# Scaling numerical columns
scaler = StandardScaler()
df[numerical_columns] = scaler.fit_transform(df[numerical_columns])

# Encoding label columns
label_encoders = {}
for col in label_columns:
    label_encoders[col] = LabelEncoder()
    df[col] = label_encoders[col].fit_transform(df[col])

# Define feature columns and target column
x = numerical_columns + label_columns
y = ['label']

# Convert data to PyTorch tensors
numerical_data = torch.tensor(df[x].values, dtype=torch.float32)
label_tensors = torch.tensor(df[y].values, dtype=torch.long)

# Create dataset
dataset = TensorDataset(numerical_data, label_tensors)

# Define batch size and split sizes
batch_size = 64
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size

# Split dataset into train and validation sets
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

# Create data loaders
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# Define the LSTMClassifier class
class LSTMClassifier(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes):
        super(LSTMClassifier, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        out, _ = self.lstm(x)
        out = self.fc(out[:, -1, :])
        return out

model = LSTMClassifier(input_size=len(x), hidden_size=128, num_layers=2, num_classes=2)  # Assuming 2 classes for binary classification
criterion = FocalLoss(gamma=2, alpha=None, reduction='mean')  # Focal Loss
optimizer = optim.Adam(model.parameters(), lr=0.0001)

# Inside the training loop
for inputs, labels in train_loader:
    optimizer.zero_grad()
    inputs = inputs.unsqueeze(1)
    outputs = model(inputs)
    outputs = F.softmax(outputs, dim=1)
    labels = labels.squeeze(1)


    # Calculating loss
    loss = criterion(outputs, labels)

    # Backpropagation
    loss.backward()
    optimizer.step()

    # Calculate accuracy
    _, predicted = torch.max(outputs, 1)
    correct = (predicted == labels).sum().item()
    total = labels.size(0)
    accuracy = correct / total

    accuracy_percent = accuracy * 100

    print("Loss:", loss.item())
    print("Accuracy:", accuracy_percent,"%")

print('hi')

