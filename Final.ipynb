{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOo1oLom4g0oQ3dAbwafSeK"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bG0QAp_1FMjL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgcG-qVYFapD",
        "outputId": "2a7da75c-7246-41a7-e8e6-665e744dd4c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/DataEBPF.csv')"
      ],
      "metadata": {
        "id": "naARnSeRFdJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "Tm-4PzfgFe1K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ad2b7b1-d9bb-4413-ccf9-63c63b2e0fd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 82332 entries, 0 to 82331\n",
            "Data columns (total 18 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   id               82332 non-null  int64  \n",
            " 1   dur              82332 non-null  float64\n",
            " 2   proto            82332 non-null  object \n",
            " 3   service          82332 non-null  object \n",
            " 4   spkts            82332 non-null  int64  \n",
            " 5   dpkts            82332 non-null  int64  \n",
            " 6   sbytes           82332 non-null  int64  \n",
            " 7   dbytes           82332 non-null  int64  \n",
            " 8   sinpkt           82332 non-null  float64\n",
            " 9   dinpkt           82332 non-null  float64\n",
            " 10  tcprtt           82332 non-null  float64\n",
            " 11  synack           82332 non-null  float64\n",
            " 12  ackdat           82332 non-null  float64\n",
            " 13  smean            82332 non-null  int64  \n",
            " 14  dmean            82332 non-null  int64  \n",
            " 15  is_sm_ips_ports  82332 non-null  int64  \n",
            " 16  attack_cat       82332 non-null  object \n",
            " 17  label            82332 non-null  int64  \n",
            "dtypes: float64(6), int64(9), object(3)\n",
            "memory usage: 11.3+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.attack_cat.value_counts()\n",
        "df.label.value_counts()"
      ],
      "metadata": {
        "id": "KSgqkVZxFhm5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03091040-1f4d-46a1-c156-b5a1098ee60e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "1    45332\n",
              "0    37000\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "#numerical_columns = ['dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'sinpkt', 'dinpkt', 'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'is_sm_ips_ports', 'label']  # List your numerical column names here\n",
        "#label_columns = ['proto', 'service', 'attack_cat']\n",
        "\n",
        "#numerical columns (scaling)\n",
        "#scaler = StandardScaler()\n",
        "#df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
        "\n",
        "#label columns (encoding)\n",
        "#label_encoders = {}\n",
        "#for col in label_columns:\n",
        "#    label_encoders[col] = LabelEncoder()\n",
        "#    df[col] = label_encoders[col].fit_transform(df[col])\n",
        "\n",
        "\n",
        "#numerical_data = torch.tensor(df[numerical_columns].values, dtype=torch.float32)\n",
        "\n",
        "#label_tensors = [torch.tensor(df[col].values, dtype=torch.long) for col in label_columns]\n",
        "\n",
        "#labels_combined = torch.cat(label_tensors, dim=0)\n",
        "#labels_combined = labels_combined.view(-1, 3)\n",
        "\n",
        "#dataset = TensorDataset(numerical_data, labels_combined)\n",
        "df['label']\n"
      ],
      "metadata": {
        "id": "3Fexf4t3Flvq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e3d349b-681d-4785-e322-9d04ee8ec421"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        0\n",
              "1        0\n",
              "2        0\n",
              "3        0\n",
              "4        0\n",
              "        ..\n",
              "82327    0\n",
              "82328    0\n",
              "82329    0\n",
              "82330    0\n",
              "82331    0\n",
              "Name: label, Length: 82332, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "numerical_columns = ['dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'sinpkt', 'dinpkt', 'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'is_sm_ips_ports']  # List your numerical column names here\n",
        "label_columns = ['proto', 'service', 'attack_cat']\n",
        "\n",
        "#numerical columns (scaling)\n",
        "scaler = StandardScaler()\n",
        "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
        "\n",
        "#label columns (encoding)\n",
        "label_encoders = {}\n",
        "for col in label_columns:\n",
        "    label_encoders[col] = LabelEncoder()\n",
        "    df[col] = label_encoders[col].fit_transform(df[col])\n",
        "\n",
        "x=['dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'sinpkt', 'dinpkt', 'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'is_sm_ips_ports', 'attack_cat','proto', 'service']\n",
        "y=['label']\n",
        "print(df['label'])\n",
        "#X_value=df[x]\n",
        "#Y_value=df[y]\n",
        "\n",
        "#print(df[numerical_columns])\n",
        "#print(df[label_columns])\n",
        "#print(df[numerical_columns].dtypes)\n",
        "\n",
        "#X_data = torch.tensor(X_value.values, dtype=torch.float32)\n",
        "\n",
        "#Y_data= torch.tensor(Y_value.values, dtype=torch.long)\n",
        "#Y_data=Y_data.view(-1,1)\n",
        "\n",
        "#X_data.shape\n",
        "#Y_data.shape\n",
        "#Y_data.shape\n",
        "#labels_combined = torch.cat(label_tensors, dim=0)\n",
        "#labels_combined = labels_combined.view(-1, 3)"
      ],
      "metadata": {
        "id": "o5kmaXJFFnkL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afaa17bf-7d16-4721-ea71-e403d54c09cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        0\n",
            "1        0\n",
            "2        0\n",
            "3        0\n",
            "4        0\n",
            "        ..\n",
            "82327    0\n",
            "82328    0\n",
            "82329    0\n",
            "82330    0\n",
            "82331    0\n",
            "Name: label, Length: 82332, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "numerical_data = torch.tensor(df[x].values, dtype=torch.float32)\n",
        "label_tensors = torch.tensor(df[y].values, dtype=torch.long)\n",
        "\n",
        "#labels_combined = torch.cat(label_tensors, dim=0)\n",
        "#labels_combined = labels_combined.view(-1, 3)\n",
        "\n",
        "datasett = TensorDataset(numerical_data, label_tensors)"
      ],
      "metadata": {
        "id": "Hdu1ppLvFqNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(label_tensors)"
      ],
      "metadata": {
        "id": "bTLqMQb0Frqq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe6796d4-95ed-4484-c833-f76ebeca5213"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0],\n",
            "        [0],\n",
            "        [0],\n",
            "        ...,\n",
            "        [0],\n",
            "        [0],\n",
            "        [0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(numerical_data.shape)\n",
        "print(label_tensors.shape)\n"
      ],
      "metadata": {
        "id": "xxqBPCVOFt8S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "787486f2-06d8-4ff9-80f9-db9045928d04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([82332, 16])\n",
            "torch.Size([82332, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "e1ld7KTrFwSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        print(out.shape)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# Assuming you already have your data loaded into numerical_data and label_tensors\n",
        "\n",
        "# Creating a dataset\n",
        "dataset = TensorDataset(numerical_data, label_tensors)\n",
        "\n",
        "# Define batch size and split sizes\n",
        "batch_size = 64\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Splitting the dataset into train and validation sets\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Creating data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Model, criterion, and optimizer initialization\n",
        "model = LSTMClassifier(input_size=16, hidden_size=128, num_layers=2, num_classes=1)  # Adjust num_classes accordingly\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.02)"
      ],
      "metadata": {
        "id": "5xNIphlOHegi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for inputs, labels in train_loader:\n",
        "    optimizer.zero_grad()\n",
        "    inputs = inputs.unsqueeze(1)\n",
        "    outputs = model(inputs)\n",
        "\n",
        "\n",
        "    outputs = F.softmax(outputs, dim=1)\n",
        "\n",
        "    print(\"Outputs:\", outputs)\n",
        "    print(\"Labels:\", labels)\n",
        "    #  Calculating accuracy of the model\n",
        "\n",
        "    _, predicted = torch.max(outputs, 1)  # Get the index of the maximum value\n",
        "    correct = (predicted == labels).sum().item()  # Count correct predictions\n",
        "    total = labels.size(0)  # Get the total number of labels\n",
        "    accuracy = correct / total\n",
        "\n",
        "\n",
        "    print(\"Outputs:\", outputs)\n",
        "    print(\"Labels:\", labels)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "\n",
        "    labels = labels.float()  # Convert labels to float for loss calculation (if needed)\n",
        "\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "    #labels = labels.float()\n",
        "\n",
        "\n",
        "\n",
        "    #loss = criterion(outputs, labels)\n",
        "\n",
        "    #loss.backward()\n",
        "    #optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "id": "0ZipOvAlFy1l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2218a4df-91a8-436b-b114-032860f08bc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0]])\n",
            "Accuracy: 23.0\n",
            "torch.Size([64, 1, 128])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0]])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0]])\n",
            "Accuracy: 27.0\n",
            "torch.Size([64, 1, 128])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0]])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0]])\n",
            "Accuracy: 27.0\n",
            "torch.Size([64, 1, 128])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1]])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1]])\n",
            "Accuracy: 24.0\n",
            "torch.Size([64, 1, 128])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0]])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0]])\n",
            "Accuracy: 31.0\n",
            "torch.Size([64, 1, 128])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0]])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0]])\n",
            "Accuracy: 29.0\n",
            "torch.Size([64, 1, 128])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1]])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1]])\n",
            "Accuracy: 21.0\n",
            "torch.Size([64, 1, 128])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0]])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0]])\n",
            "Accuracy: 30.0\n",
            "torch.Size([64, 1, 128])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0]])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0]])\n",
            "Accuracy: 27.0\n",
            "torch.Size([64, 1, 128])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0]])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0]])\n",
            "Accuracy: 25.0\n",
            "torch.Size([64, 1, 128])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1]])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1]])\n",
            "Accuracy: 28.0\n",
            "torch.Size([64, 1, 128])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1]])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1]])\n",
            "Accuracy: 29.0\n",
            "torch.Size([64, 1, 128])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1]])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1]])\n",
            "Accuracy: 28.0\n",
            "torch.Size([64, 1, 128])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1]])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1]])\n",
            "Accuracy: 27.0\n",
            "torch.Size([64, 1, 128])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0]])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0]])\n",
            "Accuracy: 31.0\n",
            "torch.Size([64, 1, 128])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0]])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0]])\n",
            "Accuracy: 23.0\n",
            "torch.Size([64, 1, 128])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1]])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1]])\n",
            "Accuracy: 30.0\n",
            "torch.Size([64, 1, 128])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0]])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0]])\n",
            "Accuracy: 28.0\n",
            "torch.Size([64, 1, 128])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1]])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1]])\n",
            "Accuracy: 29.0\n",
            "torch.Size([64, 1, 128])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1]])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1]])\n",
            "Accuracy: 34.0\n",
            "torch.Size([9, 1, 128])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1]])\n",
            "Outputs: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
            "Labels: tensor([[0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1]])\n",
            "Accuracy: 6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kkumg-VIGkcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BURRRR"
      ],
      "metadata": {
        "id": "dkooFY1VJF-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "\n",
        "# Define the Focal Loss class\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2, alpha=None, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        ce_loss = F.cross_entropy(input, target, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
        "\n",
        "        if self.alpha is not None:\n",
        "            focal_loss = self.alpha * focal_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/DataEBPF.csv')\n",
        "\n",
        "# Define numerical and label columns\n",
        "numerical_columns = ['dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'sinpkt', 'dinpkt', 'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'is_sm_ips_ports']\n",
        "label_columns = ['proto', 'service', 'attack_cat']\n",
        "\n",
        "# Scaling numerical columns\n",
        "scaler = StandardScaler()\n",
        "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
        "\n",
        "# Encoding label columns\n",
        "label_encoders = {}\n",
        "for col in label_columns:\n",
        "    label_encoders[col] = LabelEncoder()\n",
        "    df[col] = label_encoders[col].fit_transform(df[col])\n",
        "\n",
        "# Define feature columns and target column\n",
        "x = numerical_columns + label_columns\n",
        "y = ['label']\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "numerical_data = torch.tensor(df[x].values, dtype=torch.float32)\n",
        "label_tensors = torch.tensor(df[y].values, dtype=torch.long)\n",
        "\n",
        "# Create dataset\n",
        "dataset = TensorDataset(numerical_data, label_tensors)\n",
        "\n",
        "# Define batch size and split sizes\n",
        "batch_size = 64\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Split dataset into train and validation sets\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define the LSTMClassifier class\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# Model, criterion, and optimizer initialization\n",
        "model = LSTMClassifier(input_size=len(x), hidden_size=128, num_layers=2, num_classes=2)  # Assuming 2 classes for binary classification\n",
        "criterion = FocalLoss(gamma=2, alpha=None, reduction='mean')  # Focal Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDIsCcnWJEwi",
        "outputId": "188d5e38-3673-4218-ca3a-30af6e4759f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Loss: 0.17493335902690887\n",
            "Accuracy: 0.390625\n",
            "Loss: 0.17445138096809387\n",
            "Accuracy: 0.390625\n",
            "Loss: 0.17429059743881226\n",
            "Accuracy: 0.390625\n",
            "Loss: 0.17348015308380127\n",
            "Accuracy: 0.453125\n",
            "Loss: 0.1727587878704071\n",
            "Accuracy: 0.578125\n",
            "Loss: 0.17339526116847992\n",
            "Accuracy: 0.453125\n",
            "Loss: 0.17315234243869781\n",
            "Accuracy: 0.53125\n",
            "Loss: 0.1732342392206192\n",
            "Accuracy: 0.484375\n",
            "Loss: 0.17330674827098846\n",
            "Accuracy: 0.453125\n",
            "Loss: 0.17323461174964905\n",
            "Accuracy: 0.5\n",
            "Loss: 0.17316952347755432\n",
            "Accuracy: 0.515625\n",
            "Loss: 0.17296338081359863\n",
            "Accuracy: 0.578125\n",
            "Loss: 0.17302100360393524\n",
            "Accuracy: 0.53125\n",
            "Loss: 0.17292775213718414\n",
            "Accuracy: 0.546875\n",
            "Loss: 0.17347002029418945\n",
            "Accuracy: 0.453125\n",
            "Loss: 0.17274126410484314\n",
            "Accuracy: 0.5625\n",
            "Loss: 0.17268288135528564\n",
            "Accuracy: 0.5625\n",
            "Loss: 0.17214857041835785\n",
            "Accuracy: 0.625\n",
            "Loss: 0.1712857335805893\n",
            "Accuracy: 0.6875\n",
            "Loss: 0.17304305732250214\n",
            "Accuracy: 0.515625\n",
            "Loss: 0.17301534116268158\n",
            "Accuracy: 0.515625\n",
            "Loss: 0.1721053570508957\n",
            "Accuracy: 0.578125\n",
            "Loss: 0.17308799922466278\n",
            "Accuracy: 0.515625\n",
            "Loss: 0.17135240137577057\n",
            "Accuracy: 0.625\n",
            "Loss: 0.17077189683914185\n",
            "Accuracy: 0.640625\n",
            "Loss: 0.17269308865070343\n",
            "Accuracy: 0.53125\n",
            "Loss: 0.17426547408103943\n",
            "Accuracy: 0.453125\n",
            "Loss: 0.174543097615242\n",
            "Accuracy: 0.453125\n",
            "Loss: 0.17093266546726227\n",
            "Accuracy: 0.609375\n",
            "Loss: 0.17283006012439728\n",
            "Accuracy: 0.53125\n",
            "Loss: 0.17208850383758545\n",
            "Accuracy: 0.5625\n",
            "Loss: 0.17280258238315582\n",
            "Accuracy: 0.53125\n",
            "Loss: 0.1735791563987732\n",
            "Accuracy: 0.5\n",
            "Loss: 0.16856148838996887\n",
            "Accuracy: 0.6875\n",
            "Loss: 0.17157013714313507\n",
            "Accuracy: 0.578125\n",
            "Loss: 0.1700551062822342\n",
            "Accuracy: 0.625\n",
            "Loss: 0.17081522941589355\n",
            "Accuracy: 0.59375\n",
            "Loss: 0.1702638864517212\n",
            "Accuracy: 0.609375\n",
            "Loss: 0.17468714714050293\n",
            "Accuracy: 0.46875\n",
            "Loss: 0.16972103714942932\n",
            "Accuracy: 0.625\n",
            "Loss: 0.16956837475299835\n",
            "Accuracy: 0.625\n",
            "Loss: 0.1693548560142517\n",
            "Accuracy: 0.640625\n",
            "Loss: 0.16864794492721558\n",
            "Accuracy: 0.640625\n",
            "Loss: 0.17307135462760925\n",
            "Accuracy: 0.53125\n",
            "Loss: 0.17231790721416473\n",
            "Accuracy: 0.546875\n",
            "Loss: 0.170815572142601\n",
            "Accuracy: 0.578125\n",
            "Loss: 0.16951431334018707\n",
            "Accuracy: 0.625\n",
            "Loss: 0.17280562222003937\n",
            "Accuracy: 0.53125\n",
            "Loss: 0.17349423468112946\n",
            "Accuracy: 0.515625\n",
            "Loss: 0.17728912830352783\n",
            "Accuracy: 0.4375\n",
            "Loss: 0.1759507805109024\n",
            "Accuracy: 0.46875\n",
            "Loss: 0.17365017533302307\n",
            "Accuracy: 0.53125\n",
            "Loss: 0.176601380109787\n",
            "Accuracy: 0.453125\n",
            "Loss: 0.1691703498363495\n",
            "Accuracy: 0.609375\n",
            "Loss: 0.17021232843399048\n",
            "Accuracy: 0.59375\n",
            "Loss: 0.1708824187517166\n",
            "Accuracy: 0.578125\n",
            "Loss: 0.17286019027233124\n",
            "Accuracy: 0.53125\n",
            "Loss: 0.17326311767101288\n",
            "Accuracy: 0.515625\n",
            "Loss: 0.16823498904705048\n",
            "Accuracy: 0.640625\n",
            "Loss: 0.16526368260383606\n",
            "Accuracy: 0.703125\n",
            "Loss: 0.17286358773708344\n",
            "Accuracy: 0.53125\n",
            "Loss: 0.17550039291381836\n",
            "Accuracy: 0.46875\n",
            "Loss: 0.16576620936393738\n",
            "Accuracy: 0.6875\n",
            "Loss: 0.17342159152030945\n",
            "Accuracy: 0.515625\n",
            "Loss: 0.1728173941373825\n",
            "Accuracy: 0.546875\n",
            "Loss: 0.1682417094707489\n",
            "Accuracy: 0.625\n",
            "Loss: 0.1711624264717102\n",
            "Accuracy: 0.5625\n",
            "Loss: 0.17189913988113403\n",
            "Accuracy: 0.546875\n",
            "Loss: 0.17205826938152313\n",
            "Accuracy: 0.546875\n",
            "Loss: 0.1701304167509079\n",
            "Accuracy: 0.59375\n",
            "Loss: 0.17200246453285217\n",
            "Accuracy: 0.546875\n",
            "Loss: 0.16893558204174042\n",
            "Accuracy: 0.609375\n",
            "Loss: 0.16849802434444427\n",
            "Accuracy: 0.625\n",
            "Loss: 0.17025712132453918\n",
            "Accuracy: 0.578125\n",
            "Loss: 0.17011810839176178\n",
            "Accuracy: 0.578125\n",
            "Loss: 0.16986510157585144\n",
            "Accuracy: 0.59375\n",
            "Loss: 0.16933739185333252\n",
            "Accuracy: 0.59375\n",
            "Loss: 0.17010797560214996\n",
            "Accuracy: 0.578125\n",
            "Loss: 0.1681954711675644\n",
            "Accuracy: 0.625\n",
            "Loss: 0.1718587875366211\n",
            "Accuracy: 0.546875\n",
            "Loss: 0.1703181266784668\n",
            "Accuracy: 0.578125\n",
            "Loss: 0.17058460414409637\n",
            "Accuracy: 0.578125\n",
            "Loss: 0.1757708638906479\n",
            "Accuracy: 0.46875\n",
            "Loss: 0.16950231790542603\n",
            "Accuracy: 0.578125\n",
            "Loss: 0.17099912464618683\n",
            "Accuracy: 0.5625\n",
            "Loss: 0.16661396622657776\n",
            "Accuracy: 0.640625\n",
            "Loss: 0.17629003524780273\n",
            "Accuracy: 0.46875\n",
            "Loss: 0.1707231104373932\n",
            "Accuracy: 0.5625\n",
            "Loss: 0.1735966056585312\n",
            "Accuracy: 0.515625\n",
            "Loss: 0.17172062397003174\n",
            "Accuracy: 0.546875\n",
            "Loss: 0.17651186883449554\n",
            "Accuracy: 0.46875\n",
            "Loss: 0.1695476621389389\n",
            "Accuracy: 0.59375\n",
            "Loss: 0.17006909847259521\n",
            "Accuracy: 0.578125\n",
            "Loss: 0.17599572241306305\n",
            "Accuracy: 0.46875\n",
            "Loss: 0.16894984245300293\n",
            "Accuracy: 0.59375\n",
            "Loss: 0.17421208322048187\n",
            "Accuracy: 0.5\n",
            "Loss: 0.16470646858215332\n",
            "Accuracy: 0.6875\n",
            "Loss: 0.1737017184495926\n",
            "Accuracy: 0.5\n",
            "Loss: 0.17712357640266418\n",
            "Accuracy: 0.421875\n",
            "Loss: 0.17142254114151\n",
            "Accuracy: 0.546875\n",
            "Loss: 0.17231136560440063\n",
            "Accuracy: 0.53125\n",
            "Loss: 0.16809821128845215\n",
            "Accuracy: 0.609375\n",
            "Loss: 0.17105749249458313\n",
            "Accuracy: 0.546875\n",
            "Loss: 0.16705362498760223\n",
            "Accuracy: 0.640625\n",
            "Loss: 0.17092162370681763\n",
            "Accuracy: 0.546875\n",
            "Loss: 0.17006811499595642\n",
            "Accuracy: 0.578125\n",
            "Loss: 0.17214173078536987\n",
            "Accuracy: 0.515625\n",
            "Loss: 0.16879595816135406\n",
            "Accuracy: 0.59375\n",
            "Loss: 0.16898687183856964\n",
            "Accuracy: 0.59375\n",
            "Loss: 0.17088758945465088\n",
            "Accuracy: 0.546875\n",
            "Loss: 0.16980454325675964\n",
            "Accuracy: 0.5625\n",
            "Loss: 0.1681963950395584\n",
            "Accuracy: 0.609375\n",
            "Loss: 0.16892290115356445\n",
            "Accuracy: 0.59375\n",
            "Loss: 0.16807931661605835\n",
            "Accuracy: 0.609375\n",
            "Loss: 0.17634665966033936\n",
            "Accuracy: 0.40625\n",
            "Loss: 0.1692918837070465\n",
            "Accuracy: 0.578125\n",
            "Loss: 0.16844679415225983\n",
            "Accuracy: 0.609375\n",
            "Loss: 0.16987885534763336\n",
            "Accuracy: 0.5625\n",
            "Loss: 0.17175143957138062\n",
            "Accuracy: 0.53125\n",
            "Loss: 0.16636916995048523\n",
            "Accuracy: 0.640625\n",
            "Loss: 0.1717160940170288\n",
            "Accuracy: 0.515625\n",
            "Loss: 0.1652088761329651\n",
            "Accuracy: 0.671875\n",
            "Loss: 0.16958528757095337\n",
            "Accuracy: 0.578125\n",
            "Loss: 0.16962872445583344\n",
            "Accuracy: 0.5625\n",
            "Loss: 0.173051118850708\n",
            "Accuracy: 0.484375\n",
            "Loss: 0.16519665718078613\n",
            "Accuracy: 0.65625\n",
            "Loss: 0.16652441024780273\n",
            "Accuracy: 0.625\n",
            "Loss: 0.17224189639091492\n",
            "Accuracy: 0.5\n",
            "Loss: 0.17145949602127075\n",
            "Accuracy: 0.53125\n",
            "Loss: 0.17620863020420074\n",
            "Accuracy: 0.4375\n",
            "Loss: 0.167184978723526\n",
            "Accuracy: 0.609375\n",
            "Loss: 0.167207270860672\n",
            "Accuracy: 0.609375\n",
            "Loss: 0.17137554287910461\n",
            "Accuracy: 0.53125\n",
            "Loss: 0.16914454102516174\n",
            "Accuracy: 0.578125\n",
            "Loss: 0.17361582815647125\n",
            "Accuracy: 0.46875\n",
            "Loss: 0.17257453501224518\n",
            "Accuracy: 0.5\n",
            "Loss: 0.1701294332742691\n",
            "Accuracy: 0.546875\n",
            "Loss: 0.16878946125507355\n",
            "Accuracy: 0.578125\n",
            "Loss: 0.16496287286281586\n",
            "Accuracy: 0.625\n",
            "Loss: 0.16990843415260315\n",
            "Accuracy: 0.546875\n",
            "Loss: 0.16556891798973083\n",
            "Accuracy: 0.640625\n",
            "Loss: 0.16749949753284454\n",
            "Accuracy: 0.59375\n",
            "Loss: 0.17069891095161438\n",
            "Accuracy: 0.53125\n",
            "Loss: 0.17412805557250977\n",
            "Accuracy: 0.453125\n",
            "Loss: 0.1708873212337494\n",
            "Accuracy: 0.515625\n",
            "Loss: 0.1639416515827179\n",
            "Accuracy: 0.65625\n",
            "Loss: 0.1634248048067093\n",
            "Accuracy: 0.671875\n",
            "Loss: 0.17203961312770844\n",
            "Accuracy: 0.484375\n",
            "Loss: 0.16367927193641663\n",
            "Accuracy: 0.65625\n",
            "Loss: 0.16573259234428406\n",
            "Accuracy: 0.625\n",
            "Loss: 0.16852982342243195\n",
            "Accuracy: 0.578125\n",
            "Loss: 0.17010241746902466\n",
            "Accuracy: 0.53125\n",
            "Loss: 0.17227046191692352\n",
            "Accuracy: 0.484375\n",
            "Loss: 0.16648495197296143\n",
            "Accuracy: 0.59375\n",
            "Loss: 0.17087960243225098\n",
            "Accuracy: 0.5\n",
            "Loss: 0.16638809442520142\n",
            "Accuracy: 0.59375\n",
            "Loss: 0.16443169116973877\n",
            "Accuracy: 0.625\n",
            "Loss: 0.1666928231716156\n",
            "Accuracy: 0.578125\n",
            "Loss: 0.16615252196788788\n",
            "Accuracy: 0.59375\n",
            "Loss: 0.17472659051418304\n",
            "Accuracy: 0.4375\n",
            "Loss: 0.1653897911310196\n",
            "Accuracy: 0.59375\n",
            "Loss: 0.171382337808609\n",
            "Accuracy: 0.484375\n",
            "Loss: 0.171901673078537\n",
            "Accuracy: 0.46875\n",
            "Loss: 0.1709480583667755\n",
            "Accuracy: 0.5\n",
            "Loss: 0.16569764912128448\n",
            "Accuracy: 0.578125\n",
            "Loss: 0.1673874706029892\n",
            "Accuracy: 0.546875\n",
            "Loss: 0.16672560572624207\n",
            "Accuracy: 0.578125\n",
            "Loss: 0.1665385365486145\n",
            "Accuracy: 0.578125\n",
            "Loss: 0.16806291043758392\n",
            "Accuracy: 0.515625\n",
            "Loss: 0.16283495724201202\n",
            "Accuracy: 0.671875\n",
            "Loss: 0.16703209280967712\n",
            "Accuracy: 0.5625\n",
            "Loss: 0.16656427085399628\n",
            "Accuracy: 0.5625\n",
            "Loss: 0.1651557832956314\n",
            "Accuracy: 0.609375\n",
            "Loss: 0.1655578762292862\n",
            "Accuracy: 0.609375\n",
            "Loss: 0.1613021343946457\n",
            "Accuracy: 0.671875\n",
            "Loss: 0.16235928237438202\n",
            "Accuracy: 0.640625\n",
            "Loss: 0.1681111454963684\n",
            "Accuracy: 0.515625\n",
            "Loss: 0.16433382034301758\n",
            "Accuracy: 0.609375\n",
            "Loss: 0.1690630167722702\n",
            "Accuracy: 0.5\n",
            "Loss: 0.1642518788576126\n",
            "Accuracy: 0.59375\n",
            "Loss: 0.1644223928451538\n",
            "Accuracy: 0.609375\n",
            "Loss: 0.16504214704036713\n",
            "Accuracy: 0.59375\n",
            "Loss: 0.1693139374256134\n",
            "Accuracy: 0.484375\n",
            "Loss: 0.16659817099571228\n",
            "Accuracy: 0.546875\n",
            "Loss: 0.16287273168563843\n",
            "Accuracy: 0.59375\n",
            "Loss: 0.16665354371070862\n",
            "Accuracy: 0.546875\n",
            "Loss: 0.16552110016345978\n",
            "Accuracy: 0.5625\n",
            "Loss: 0.16481748223304749\n",
            "Accuracy: 0.546875\n",
            "Loss: 0.16541169583797455\n",
            "Accuracy: 0.578125\n",
            "Loss: 0.1677784025669098\n",
            "Accuracy: 0.53125\n",
            "Loss: 0.1685781180858612\n",
            "Accuracy: 0.5\n",
            "Loss: 0.1647651493549347\n",
            "Accuracy: 0.5625\n",
            "Loss: 0.16343723237514496\n",
            "Accuracy: 0.578125\n",
            "Loss: 0.16679716110229492\n",
            "Accuracy: 0.546875\n",
            "Loss: 0.16952750086784363\n",
            "Accuracy: 0.5\n",
            "Loss: 0.1664077341556549\n",
            "Accuracy: 0.546875\n",
            "Loss: 0.16943949460983276\n",
            "Accuracy: 0.46875\n",
            "Loss: 0.1660226583480835\n",
            "Accuracy: 0.5625\n",
            "Loss: 0.1654578149318695\n",
            "Accuracy: 0.5625\n",
            "Loss: 0.1637973040342331\n",
            "Accuracy: 0.546875\n",
            "Loss: 0.16500744223594666\n",
            "Accuracy: 0.5\n",
            "Loss: 0.16404293477535248\n",
            "Accuracy: 0.53125\n",
            "Loss: 0.1627061516046524\n",
            "Accuracy: 0.578125\n",
            "Loss: 0.16203798353672028\n",
            "Accuracy: 0.671875\n",
            "Loss: 0.16406014561653137\n",
            "Accuracy: 0.59375\n",
            "Loss: 0.16051435470581055\n",
            "Accuracy: 0.703125\n",
            "Loss: 0.16576306521892548\n",
            "Accuracy: 0.6875\n",
            "Loss: 0.16392721235752106\n",
            "Accuracy: 0.703125\n",
            "Loss: 0.16245047748088837\n",
            "Accuracy: 0.6875\n",
            "Loss: 0.1586335152387619\n",
            "Accuracy: 0.828125\n",
            "Loss: 0.165158212184906\n",
            "Accuracy: 0.703125\n",
            "Loss: 0.16394630074501038\n",
            "Accuracy: 0.734375\n",
            "Loss: 0.1611955761909485\n",
            "Accuracy: 0.75\n",
            "Loss: 0.1629997193813324\n",
            "Accuracy: 0.765625\n",
            "Loss: 0.16434341669082642\n",
            "Accuracy: 0.8125\n",
            "Loss: 0.16340294480323792\n",
            "Accuracy: 0.8125\n",
            "Loss: 0.16247884929180145\n",
            "Accuracy: 0.796875\n",
            "Loss: 0.16552436351776123\n",
            "Accuracy: 0.6875\n",
            "Loss: 0.1642114818096161\n",
            "Accuracy: 0.65625\n",
            "Loss: 0.1622171700000763\n",
            "Accuracy: 0.71875\n",
            "Loss: 0.16050991415977478\n",
            "Accuracy: 0.78125\n",
            "Loss: 0.16106286644935608\n",
            "Accuracy: 0.734375\n",
            "Loss: 0.15868720412254333\n",
            "Accuracy: 0.734375\n",
            "Loss: 0.16382545232772827\n",
            "Accuracy: 0.671875\n",
            "Loss: 0.16072219610214233\n",
            "Accuracy: 0.640625\n",
            "Loss: 0.16238559782505035\n",
            "Accuracy: 0.703125\n",
            "Loss: 0.16017083823680878\n",
            "Accuracy: 0.765625\n",
            "Loss: 0.1629752218723297\n",
            "Accuracy: 0.625\n",
            "Loss: 0.1567908078432083\n",
            "Accuracy: 0.703125\n",
            "Loss: 0.16335387527942657\n",
            "Accuracy: 0.625\n",
            "Loss: 0.16318194568157196\n",
            "Accuracy: 0.625\n",
            "Loss: 0.15676811337471008\n",
            "Accuracy: 0.6875\n",
            "Loss: 0.15426212549209595\n",
            "Accuracy: 0.703125\n",
            "Loss: 0.16189397871494293\n",
            "Accuracy: 0.53125\n",
            "Loss: 0.15888206660747528\n",
            "Accuracy: 0.59375\n",
            "Loss: 0.15216149389743805\n",
            "Accuracy: 0.734375\n",
            "Loss: 0.16173797845840454\n",
            "Accuracy: 0.578125\n",
            "Loss: 0.16361543536186218\n",
            "Accuracy: 0.578125\n",
            "Loss: 0.1584092229604721\n",
            "Accuracy: 0.5625\n",
            "Loss: 0.15809665620326996\n",
            "Accuracy: 0.59375\n",
            "Loss: 0.1592630296945572\n",
            "Accuracy: 0.59375\n",
            "Loss: 0.15325269103050232\n",
            "Accuracy: 0.671875\n",
            "Loss: 0.16194669902324677\n",
            "Accuracy: 0.609375\n",
            "Loss: 0.15285450220108032\n",
            "Accuracy: 0.6875\n",
            "Loss: 0.15929380059242249\n",
            "Accuracy: 0.609375\n",
            "Loss: 0.15676964819431305\n",
            "Accuracy: 0.609375\n",
            "Loss: 0.15507960319519043\n",
            "Accuracy: 0.703125\n",
            "Loss: 0.15250855684280396\n",
            "Accuracy: 0.765625\n",
            "Loss: 0.15473425388336182\n",
            "Accuracy: 0.703125\n",
            "Loss: 0.15211117267608643\n",
            "Accuracy: 0.71875\n",
            "Loss: 0.1505475491285324\n",
            "Accuracy: 0.75\n",
            "Loss: 0.1486184000968933\n",
            "Accuracy: 0.703125\n",
            "Loss: 0.14922691881656647\n",
            "Accuracy: 0.734375\n",
            "Loss: 0.16049036383628845\n",
            "Accuracy: 0.53125\n",
            "Loss: 0.15850089490413666\n",
            "Accuracy: 0.578125\n",
            "Loss: 0.1542535275220871\n",
            "Accuracy: 0.625\n",
            "Loss: 0.15589639544487\n",
            "Accuracy: 0.625\n",
            "Loss: 0.15733887255191803\n",
            "Accuracy: 0.609375\n",
            "Loss: 0.15411056578159332\n",
            "Accuracy: 0.65625\n",
            "Loss: 0.15300434827804565\n",
            "Accuracy: 0.6875\n",
            "Loss: 0.14928624033927917\n",
            "Accuracy: 0.765625\n",
            "Loss: 0.15093065798282623\n",
            "Accuracy: 0.765625\n",
            "Loss: 0.14915332198143005\n",
            "Accuracy: 0.765625\n",
            "Loss: 0.1534789353609085\n",
            "Accuracy: 0.703125\n",
            "Loss: 0.1472962200641632\n",
            "Accuracy: 0.8125\n",
            "Loss: 0.15015996992588043\n",
            "Accuracy: 0.796875\n",
            "Loss: 0.15371912717819214\n",
            "Accuracy: 0.734375\n",
            "Loss: 0.1567387878894806\n",
            "Accuracy: 0.765625\n",
            "Loss: 0.15665096044540405\n",
            "Accuracy: 0.703125\n",
            "Loss: 0.15218235552310944\n",
            "Accuracy: 0.8125\n",
            "Loss: 0.14689621329307556\n",
            "Accuracy: 0.8125\n",
            "Loss: 0.15345793962478638\n",
            "Accuracy: 0.75\n",
            "Loss: 0.15159590542316437\n",
            "Accuracy: 0.8125\n",
            "Loss: 0.15365266799926758\n",
            "Accuracy: 0.796875\n",
            "Loss: 0.14756205677986145\n",
            "Accuracy: 0.859375\n",
            "Loss: 0.14966873824596405\n",
            "Accuracy: 0.796875\n",
            "Loss: 0.14863508939743042\n",
            "Accuracy: 0.6875\n",
            "Loss: 0.1409095972776413\n",
            "Accuracy: 0.796875\n",
            "Loss: 0.1434391289949417\n",
            "Accuracy: 0.734375\n",
            "Loss: 0.14253762364387512\n",
            "Accuracy: 0.75\n",
            "Loss: 0.14411954581737518\n",
            "Accuracy: 0.6875\n",
            "Loss: 0.1447124481201172\n",
            "Accuracy: 0.75\n",
            "Loss: 0.1491578221321106\n",
            "Accuracy: 0.625\n",
            "Loss: 0.14492803812026978\n",
            "Accuracy: 0.703125\n",
            "Loss: 0.13643647730350494\n",
            "Accuracy: 0.78125\n",
            "Loss: 0.15381895005702972\n",
            "Accuracy: 0.609375\n",
            "Loss: 0.14961124956607819\n",
            "Accuracy: 0.6875\n",
            "Loss: 0.14553576707839966\n",
            "Accuracy: 0.703125\n",
            "Loss: 0.15141087770462036\n",
            "Accuracy: 0.625\n",
            "Loss: 0.14457948505878448\n",
            "Accuracy: 0.734375\n",
            "Loss: 0.13625478744506836\n",
            "Accuracy: 0.796875\n",
            "Loss: 0.14460664987564087\n",
            "Accuracy: 0.734375\n",
            "Loss: 0.14552520215511322\n",
            "Accuracy: 0.796875\n",
            "Loss: 0.1495782434940338\n",
            "Accuracy: 0.765625\n",
            "Loss: 0.1448058933019638\n",
            "Accuracy: 0.8125\n",
            "Loss: 0.14758113026618958\n",
            "Accuracy: 0.78125\n",
            "Loss: 0.14083191752433777\n",
            "Accuracy: 0.828125\n",
            "Loss: 0.13965772092342377\n",
            "Accuracy: 0.84375\n",
            "Loss: 0.14488227665424347\n",
            "Accuracy: 0.828125\n",
            "Loss: 0.1434713900089264\n",
            "Accuracy: 0.796875\n",
            "Loss: 0.1404801309108734\n",
            "Accuracy: 0.78125\n",
            "Loss: 0.14105893671512604\n",
            "Accuracy: 0.78125\n",
            "Loss: 0.14219865202903748\n",
            "Accuracy: 0.8125\n",
            "Loss: 0.14719544351100922\n",
            "Accuracy: 0.765625\n",
            "Loss: 0.13591432571411133\n",
            "Accuracy: 0.8125\n",
            "Loss: 0.14174172282218933\n",
            "Accuracy: 0.796875\n",
            "Loss: 0.13401609659194946\n",
            "Accuracy: 0.875\n",
            "Loss: 0.1370321810245514\n",
            "Accuracy: 0.78125\n",
            "Loss: 0.1378953754901886\n",
            "Accuracy: 0.78125\n",
            "Loss: 0.1458713710308075\n",
            "Accuracy: 0.78125\n",
            "Loss: 0.12773500382900238\n",
            "Accuracy: 0.8125\n",
            "Loss: 0.1278282105922699\n",
            "Accuracy: 0.859375\n",
            "Loss: 0.1350269466638565\n",
            "Accuracy: 0.703125\n",
            "Loss: 0.1321038454771042\n",
            "Accuracy: 0.75\n",
            "Loss: 0.1277998685836792\n",
            "Accuracy: 0.796875\n",
            "Loss: 0.13403423130512238\n",
            "Accuracy: 0.734375\n",
            "Loss: 0.13686206936836243\n",
            "Accuracy: 0.703125\n",
            "Loss: 0.14383000135421753\n",
            "Accuracy: 0.6875\n",
            "Loss: 0.1437082439661026\n",
            "Accuracy: 0.65625\n",
            "Loss: 0.12823262810707092\n",
            "Accuracy: 0.78125\n",
            "Loss: 0.13865256309509277\n",
            "Accuracy: 0.734375\n",
            "Loss: 0.1340399980545044\n",
            "Accuracy: 0.765625\n",
            "Loss: 0.12366421520709991\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.13020329177379608\n",
            "Accuracy: 0.828125\n",
            "Loss: 0.141867995262146\n",
            "Accuracy: 0.765625\n",
            "Loss: 0.1265195608139038\n",
            "Accuracy: 0.875\n",
            "Loss: 0.1257176697254181\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.1357969045639038\n",
            "Accuracy: 0.796875\n",
            "Loss: 0.13499930500984192\n",
            "Accuracy: 0.765625\n",
            "Loss: 0.1413087695837021\n",
            "Accuracy: 0.75\n",
            "Loss: 0.12974536418914795\n",
            "Accuracy: 0.8125\n",
            "Loss: 0.1368986815214157\n",
            "Accuracy: 0.78125\n",
            "Loss: 0.13532260060310364\n",
            "Accuracy: 0.78125\n",
            "Loss: 0.13996538519859314\n",
            "Accuracy: 0.734375\n",
            "Loss: 0.13996592164039612\n",
            "Accuracy: 0.734375\n",
            "Loss: 0.1444677859544754\n",
            "Accuracy: 0.734375\n",
            "Loss: 0.13532428443431854\n",
            "Accuracy: 0.765625\n",
            "Loss: 0.12924402952194214\n",
            "Accuracy: 0.78125\n",
            "Loss: 0.11670732498168945\n",
            "Accuracy: 0.859375\n",
            "Loss: 0.13810984790325165\n",
            "Accuracy: 0.8125\n",
            "Loss: 0.14569786190986633\n",
            "Accuracy: 0.671875\n",
            "Loss: 0.11148101836442947\n",
            "Accuracy: 0.875\n",
            "Loss: 0.12389002740383148\n",
            "Accuracy: 0.84375\n",
            "Loss: 0.12628643214702606\n",
            "Accuracy: 0.84375\n",
            "Loss: 0.13344287872314453\n",
            "Accuracy: 0.765625\n",
            "Loss: 0.11849366873502731\n",
            "Accuracy: 0.875\n",
            "Loss: 0.12338530272245407\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.12330113351345062\n",
            "Accuracy: 0.84375\n",
            "Loss: 0.11952824890613556\n",
            "Accuracy: 0.828125\n",
            "Loss: 0.12832041084766388\n",
            "Accuracy: 0.78125\n",
            "Loss: 0.12563112378120422\n",
            "Accuracy: 0.8125\n",
            "Loss: 0.11421914398670197\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.12868128716945648\n",
            "Accuracy: 0.78125\n",
            "Loss: 0.11092472076416016\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.1214904934167862\n",
            "Accuracy: 0.765625\n",
            "Loss: 0.1276773363351822\n",
            "Accuracy: 0.796875\n",
            "Loss: 0.13680732250213623\n",
            "Accuracy: 0.671875\n",
            "Loss: 0.12342190742492676\n",
            "Accuracy: 0.8125\n",
            "Loss: 0.11795324832201004\n",
            "Accuracy: 0.8125\n",
            "Loss: 0.12530089914798737\n",
            "Accuracy: 0.734375\n",
            "Loss: 0.11680732667446136\n",
            "Accuracy: 0.84375\n",
            "Loss: 0.11559014767408371\n",
            "Accuracy: 0.78125\n",
            "Loss: 0.12781114876270294\n",
            "Accuracy: 0.78125\n",
            "Loss: 0.11944714188575745\n",
            "Accuracy: 0.828125\n",
            "Loss: 0.14363068342208862\n",
            "Accuracy: 0.703125\n",
            "Loss: 0.12911640107631683\n",
            "Accuracy: 0.75\n",
            "Loss: 0.11251336336135864\n",
            "Accuracy: 0.828125\n",
            "Loss: 0.11976831406354904\n",
            "Accuracy: 0.875\n",
            "Loss: 0.11979474127292633\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.10627587139606476\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.12129734456539154\n",
            "Accuracy: 0.84375\n",
            "Loss: 0.13044512271881104\n",
            "Accuracy: 0.734375\n",
            "Loss: 0.1247776448726654\n",
            "Accuracy: 0.796875\n",
            "Loss: 0.11708221584558487\n",
            "Accuracy: 0.78125\n",
            "Loss: 0.12781362235546112\n",
            "Accuracy: 0.828125\n",
            "Loss: 0.11107257008552551\n",
            "Accuracy: 0.828125\n",
            "Loss: 0.11342880129814148\n",
            "Accuracy: 0.8125\n",
            "Loss: 0.1288098692893982\n",
            "Accuracy: 0.765625\n",
            "Loss: 0.12513762712478638\n",
            "Accuracy: 0.75\n",
            "Loss: 0.12080754339694977\n",
            "Accuracy: 0.765625\n",
            "Loss: 0.12374182790517807\n",
            "Accuracy: 0.78125\n",
            "Loss: 0.11638882756233215\n",
            "Accuracy: 0.828125\n",
            "Loss: 0.1286831945180893\n",
            "Accuracy: 0.828125\n",
            "Loss: 0.1154334545135498\n",
            "Accuracy: 0.875\n",
            "Loss: 0.12905730307102203\n",
            "Accuracy: 0.8125\n",
            "Loss: 0.10686798393726349\n",
            "Accuracy: 0.875\n",
            "Loss: 0.1179381012916565\n",
            "Accuracy: 0.859375\n",
            "Loss: 0.11831461638212204\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.1094193384051323\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.10614298284053802\n",
            "Accuracy: 0.84375\n",
            "Loss: 0.11145512759685516\n",
            "Accuracy: 0.859375\n",
            "Loss: 0.10883381217718124\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.10607822984457016\n",
            "Accuracy: 0.859375\n",
            "Loss: 0.1285112053155899\n",
            "Accuracy: 0.75\n",
            "Loss: 0.13339796662330627\n",
            "Accuracy: 0.734375\n",
            "Loss: 0.12964943051338196\n",
            "Accuracy: 0.734375\n",
            "Loss: 0.1110222190618515\n",
            "Accuracy: 0.84375\n",
            "Loss: 0.11705082654953003\n",
            "Accuracy: 0.765625\n",
            "Loss: 0.11142764985561371\n",
            "Accuracy: 0.8125\n",
            "Loss: 0.09993264079093933\n",
            "Accuracy: 0.859375\n",
            "Loss: 0.12363045662641525\n",
            "Accuracy: 0.859375\n",
            "Loss: 0.10630500316619873\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.1221848875284195\n",
            "Accuracy: 0.8125\n",
            "Loss: 0.13358229398727417\n",
            "Accuracy: 0.78125\n",
            "Loss: 0.11950945854187012\n",
            "Accuracy: 0.828125\n",
            "Loss: 0.107813760638237\n",
            "Accuracy: 0.859375\n",
            "Loss: 0.11499491333961487\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.11927073448896408\n",
            "Accuracy: 0.859375\n",
            "Loss: 0.10459441691637039\n",
            "Accuracy: 0.859375\n",
            "Loss: 0.10414643585681915\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.1025601252913475\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.1171107143163681\n",
            "Accuracy: 0.84375\n",
            "Loss: 0.10888493061065674\n",
            "Accuracy: 0.859375\n",
            "Loss: 0.11682870984077454\n",
            "Accuracy: 0.796875\n",
            "Loss: 0.1079893410205841\n",
            "Accuracy: 0.84375\n",
            "Loss: 0.0854727029800415\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.12426409870386124\n",
            "Accuracy: 0.796875\n",
            "Loss: 0.12429795414209366\n",
            "Accuracy: 0.828125\n",
            "Loss: 0.11199691891670227\n",
            "Accuracy: 0.796875\n",
            "Loss: 0.11103010177612305\n",
            "Accuracy: 0.8125\n",
            "Loss: 0.0976591408252716\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.10556523501873016\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.10413838922977448\n",
            "Accuracy: 0.84375\n",
            "Loss: 0.09374235570430756\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.0923926830291748\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.1108160987496376\n",
            "Accuracy: 0.796875\n",
            "Loss: 0.09916280955076218\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.10166630893945694\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.10523059219121933\n",
            "Accuracy: 0.84375\n",
            "Loss: 0.1056351363658905\n",
            "Accuracy: 0.8125\n",
            "Loss: 0.11477428674697876\n",
            "Accuracy: 0.828125\n",
            "Loss: 0.1253776252269745\n",
            "Accuracy: 0.75\n",
            "Loss: 0.09888771921396255\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.09438452869653702\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.11039171367883682\n",
            "Accuracy: 0.875\n",
            "Loss: 0.11103934049606323\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.11646048724651337\n",
            "Accuracy: 0.875\n",
            "Loss: 0.10914541035890579\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.1006290465593338\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.09698231518268585\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.10439557582139969\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.10756713896989822\n",
            "Accuracy: 0.875\n",
            "Loss: 0.11329488456249237\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.11705327033996582\n",
            "Accuracy: 0.84375\n",
            "Loss: 0.0949847623705864\n",
            "Accuracy: 0.859375\n",
            "Loss: 0.0921679139137268\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.11106417328119278\n",
            "Accuracy: 0.84375\n",
            "Loss: 0.09601537883281708\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.10726464539766312\n",
            "Accuracy: 0.828125\n",
            "Loss: 0.10031972825527191\n",
            "Accuracy: 0.84375\n",
            "Loss: 0.10907451063394547\n",
            "Accuracy: 0.859375\n",
            "Loss: 0.0969226062297821\n",
            "Accuracy: 0.875\n",
            "Loss: 0.10637274384498596\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.09221655130386353\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.09683896601200104\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.1286952644586563\n",
            "Accuracy: 0.78125\n",
            "Loss: 0.10046053677797318\n",
            "Accuracy: 0.84375\n",
            "Loss: 0.12234288454055786\n",
            "Accuracy: 0.828125\n",
            "Loss: 0.10499685257673264\n",
            "Accuracy: 0.859375\n",
            "Loss: 0.11352751404047012\n",
            "Accuracy: 0.84375\n",
            "Loss: 0.08909368515014648\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.10068680346012115\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.1048399806022644\n",
            "Accuracy: 0.859375\n",
            "Loss: 0.10544741153717041\n",
            "Accuracy: 0.84375\n",
            "Loss: 0.12614716589450836\n",
            "Accuracy: 0.8125\n",
            "Loss: 0.07467811554670334\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.1228354424238205\n",
            "Accuracy: 0.765625\n",
            "Loss: 0.09659361839294434\n",
            "Accuracy: 0.859375\n",
            "Loss: 0.10015909373760223\n",
            "Accuracy: 0.875\n",
            "Loss: 0.12299531698226929\n",
            "Accuracy: 0.796875\n",
            "Loss: 0.09554970264434814\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.10664161294698715\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.11236405372619629\n",
            "Accuracy: 0.796875\n",
            "Loss: 0.08900367468595505\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.10107581317424774\n",
            "Accuracy: 0.828125\n",
            "Loss: 0.08361267298460007\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.08025308698415756\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.11238939315080643\n",
            "Accuracy: 0.859375\n",
            "Loss: 0.10814555734395981\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.08858267962932587\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.0923314020037651\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.10418463498353958\n",
            "Accuracy: 0.875\n",
            "Loss: 0.10073316842317581\n",
            "Accuracy: 0.875\n",
            "Loss: 0.12421338260173798\n",
            "Accuracy: 0.765625\n",
            "Loss: 0.09897761791944504\n",
            "Accuracy: 0.859375\n",
            "Loss: 0.09650367498397827\n",
            "Accuracy: 0.828125\n",
            "Loss: 0.08316557109355927\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.12119821459054947\n",
            "Accuracy: 0.78125\n",
            "Loss: 0.10632654279470444\n",
            "Accuracy: 0.828125\n",
            "Loss: 0.11539965867996216\n",
            "Accuracy: 0.84375\n",
            "Loss: 0.08427898585796356\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.09370830655097961\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.08254358172416687\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.08756110817193985\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.10652637481689453\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.09706290066242218\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.11467930674552917\n",
            "Accuracy: 0.875\n",
            "Loss: 0.09247515350580215\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.08819795399904251\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.08624696731567383\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.09927553683519363\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.0991789847612381\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.11442459374666214\n",
            "Accuracy: 0.875\n",
            "Loss: 0.08597162365913391\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.10036275535821915\n",
            "Accuracy: 0.859375\n",
            "Loss: 0.10429605841636658\n",
            "Accuracy: 0.875\n",
            "Loss: 0.09854981303215027\n",
            "Accuracy: 0.84375\n",
            "Loss: 0.12223045527935028\n",
            "Accuracy: 0.8125\n",
            "Loss: 0.09357361495494843\n",
            "Accuracy: 0.875\n",
            "Loss: 0.10590814799070358\n",
            "Accuracy: 0.828125\n",
            "Loss: 0.08493736386299133\n",
            "Accuracy: 0.875\n",
            "Loss: 0.08941392600536346\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.09559260308742523\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.11401142179965973\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.07988771796226501\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.06861502677202225\n",
            "Accuracy: 1.0\n",
            "Loss: 0.08755487948656082\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.12546609342098236\n",
            "Accuracy: 0.8125\n",
            "Loss: 0.08918894827365875\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.07980087399482727\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.10312860459089279\n",
            "Accuracy: 0.8125\n",
            "Loss: 0.0992405116558075\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.0884789377450943\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.10327774286270142\n",
            "Accuracy: 0.875\n",
            "Loss: 0.09157157689332962\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.10423459112644196\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.10597378015518188\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.10510771721601486\n",
            "Accuracy: 0.875\n",
            "Loss: 0.11647380888462067\n",
            "Accuracy: 0.84375\n",
            "Loss: 0.07777903974056244\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.09508781135082245\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.07445964962244034\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.08523909747600555\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.08691634237766266\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.10266661643981934\n",
            "Accuracy: 0.875\n",
            "Loss: 0.07093975692987442\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.09074609726667404\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.07730667293071747\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.12708327174186707\n",
            "Accuracy: 0.859375\n",
            "Loss: 0.08665837347507477\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.09021855890750885\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.07488089054822922\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.11074142158031464\n",
            "Accuracy: 0.84375\n",
            "Loss: 0.11019012331962585\n",
            "Accuracy: 0.859375\n",
            "Loss: 0.0817812904715538\n",
            "Accuracy: 0.875\n",
            "Loss: 0.085781991481781\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.11634669452905655\n",
            "Accuracy: 0.828125\n",
            "Loss: 0.09549090266227722\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.07599924504756927\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.08909890800714493\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.06647700071334839\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.10416164249181747\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.08351936936378479\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.08638820052146912\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.06412643939256668\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.09132339805364609\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.0822034552693367\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.08852982521057129\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.06541439890861511\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.08912608027458191\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.10076914727687836\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.0724640041589737\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.09378096461296082\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.07056905329227448\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.09185316413640976\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.08751901984214783\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.0972595363855362\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.06426404416561127\n",
            "Accuracy: 1.0\n",
            "Loss: 0.07564377039670944\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.07327965646982193\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.10016372054815292\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.09107143431901932\n",
            "Accuracy: 0.875\n",
            "Loss: 0.0989660769701004\n",
            "Accuracy: 0.859375\n",
            "Loss: 0.08904305845499039\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.08394638448953629\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.09520896524190903\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.10058795660734177\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.08210828900337219\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.08686069399118423\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.07380864769220352\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.09248191118240356\n",
            "Accuracy: 0.875\n",
            "Loss: 0.09057590365409851\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.07483407855033875\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.1028234139084816\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.09428324550390244\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.08313595503568649\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.09640412032604218\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.09222213923931122\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.09333709627389908\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.0895572379231453\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.08155547082424164\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.07323062419891357\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.07897574454545975\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.06707939505577087\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.09132449328899384\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.09236325323581696\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.08587079495191574\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.09209398180246353\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.09632016718387604\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.08949805796146393\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.07600222527980804\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.09450794011354446\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.07455569505691528\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.08014517277479172\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.08112791925668716\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.10273091495037079\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.08835791051387787\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.0718286782503128\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.09965327382087708\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.07033982127904892\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.08755132555961609\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.06598642468452454\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.07807710021734238\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.07961725443601608\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.09691637009382248\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.07335882633924484\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.07412751019001007\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.07972168177366257\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.08288030326366425\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.11326953768730164\n",
            "Accuracy: 0.875\n",
            "Loss: 0.10644259303808212\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.060038164258003235\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.08289007097482681\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.07583794742822647\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.09269650280475616\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.07845211029052734\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.08158260583877563\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.07430912554264069\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.08389602601528168\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.09304690361022949\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.10100740194320679\n",
            "Accuracy: 0.859375\n",
            "Loss: 0.08063995838165283\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.07529603689908981\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.0921289250254631\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.06903315335512161\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.06934860348701477\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.07177618145942688\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.05666536092758179\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.07584375888109207\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.07851586490869522\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.07050086557865143\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.07436946779489517\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.09595085680484772\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.0970156267285347\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.06919180601835251\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.11128904670476913\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.08852992951869965\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.07533686608076096\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.06653203815221786\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.08247620612382889\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.07688423991203308\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.06965717673301697\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.09325262159109116\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.07610541582107544\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.08350764960050583\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.06315012276172638\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.10083094239234924\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.07881969213485718\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.08417598903179169\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.0775262787938118\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.0700216218829155\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.07724157720804214\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.08461527526378632\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.06261631101369858\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.08696348965167999\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.08171647787094116\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.09556428343057632\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.0893300473690033\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.08879100531339645\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.09381677955389023\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.0843859612941742\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.09264422208070755\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.058974891901016235\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.09161405265331268\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.0754363164305687\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.1329864263534546\n",
            "Accuracy: 0.859375\n",
            "Loss: 0.07591050863265991\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.12155462801456451\n",
            "Accuracy: 0.84375\n",
            "Loss: 0.07693484425544739\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.09360572695732117\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.07877153903245926\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.07052502036094666\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.08236940950155258\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.09156772494316101\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.10148050636053085\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.08846716582775116\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.08131083846092224\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.07511617243289948\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.08870527148246765\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.07487823069095612\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.058085594326257706\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.07352761179208755\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.10673323273658752\n",
            "Accuracy: 0.875\n",
            "Loss: 0.06912516057491302\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.07559984922409058\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.10621581971645355\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.07036449760198593\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.10509437322616577\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.09593476355075836\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.07263171672821045\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.09179126471281052\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.08560033142566681\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.06026574969291687\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.07074252516031265\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.11200738698244095\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.07131525129079819\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.07772336155176163\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.07361604273319244\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.08065672218799591\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.07339631021022797\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.05814363807439804\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.06641412526369095\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.06259413808584213\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.08421149104833603\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.07403884828090668\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.07239452004432678\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.06374982744455338\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.08524922281503677\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.09423725306987762\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.09450500458478928\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.08616510778665543\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.0648929700255394\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.07011101394891739\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.08349932730197906\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.06326805800199509\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.07377173751592636\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.05869583413004875\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.0762997716665268\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.10981576144695282\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.1019175723195076\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.06052606552839279\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.08323454856872559\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.07950036972761154\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.09985583275556564\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.06956879794597626\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.06237228959798813\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.06915310770273209\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.07167176902294159\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.06980468332767487\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.07366050779819489\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.09036567062139511\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.08520738035440445\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.06962210685014725\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.06583989411592484\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.06319048255681992\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.09280367195606232\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.06921493262052536\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.07134068012237549\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.1255864053964615\n",
            "Accuracy: 0.859375\n",
            "Loss: 0.07090422511100769\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.07321257889270782\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.06612969934940338\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.07102181017398834\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.061763253062963486\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.09777628630399704\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.08355958759784698\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.05001794919371605\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.04988015443086624\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.12092500180006027\n",
            "Accuracy: 0.859375\n",
            "Loss: 0.07645326107740402\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.05802321806550026\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.10232783854007721\n",
            "Accuracy: 0.875\n",
            "Loss: 0.06358736753463745\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.08099400252103806\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.08128047734498978\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.07300302386283875\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.053940434008836746\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.06863976269960403\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.07657056301832199\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.04248524457216263\n",
            "Accuracy: 1.0\n",
            "Loss: 0.06276804953813553\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.06238366290926933\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.054609622806310654\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.08000952005386353\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.0526387020945549\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.06289342790842056\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.05089950188994408\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.0727996975183487\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.047077860683202744\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.06739315390586853\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.06324933469295502\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.08661956340074539\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.08146107196807861\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.06256647408008575\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.08351786434650421\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.07529519498348236\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.07332900166511536\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.08731101453304291\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.07076919078826904\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.06325297057628632\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.05125751346349716\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.05503850430250168\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.05660460516810417\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.10029656440019608\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.07292722165584564\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.0901937484741211\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.08162125945091248\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.05887482315301895\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.09665708243846893\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.08199378103017807\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.07813709229230881\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.04849610850214958\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.058808643370866776\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.08427072316408157\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.08218038082122803\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.06313684582710266\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.0604013130068779\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.04745982587337494\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.0688975602388382\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.08151287585496902\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.08005200326442719\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.10306647419929504\n",
            "Accuracy: 0.875\n",
            "Loss: 0.08863652497529984\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.0708366110920906\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.080875925719738\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.0762079730629921\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.06294265389442444\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.1076272577047348\n",
            "Accuracy: 0.859375\n",
            "Loss: 0.09614378958940506\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.07797286659479141\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.08151277154684067\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.043537940829992294\n",
            "Accuracy: 1.0\n",
            "Loss: 0.06628967821598053\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.11475668847560883\n",
            "Accuracy: 0.875\n",
            "Loss: 0.08716394007205963\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.0610884465277195\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.046872518956661224\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.05342534929513931\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.09243949502706528\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.056643180549144745\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.10635170340538025\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.06602604687213898\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.08215111494064331\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.0502798929810524\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.07747256755828857\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.0829223096370697\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.09032019972801208\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.07015959173440933\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.048902302980422974\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.0651683509349823\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.05643467605113983\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.049042001366615295\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.058856308460235596\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.07465766370296478\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.07801777124404907\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.0752217248082161\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.05573003366589546\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.07474790513515472\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.06515379995107651\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.07588823139667511\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.05453912168741226\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.10195177793502808\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.05682433769106865\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.08869677782058716\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.07392888516187668\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.07156791538000107\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.08817558735609055\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.05644792318344116\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.04752057418227196\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.046058110892772675\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.05999494343996048\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.07632511854171753\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.0645044818520546\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.03485121577978134\n",
            "Accuracy: 1.0\n",
            "Loss: 0.09426689147949219\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.07300662994384766\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.04684558883309364\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.056176502257585526\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.07668332010507584\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.03875008970499039\n",
            "Accuracy: 1.0\n",
            "Loss: 0.035300664603710175\n",
            "Accuracy: 1.0\n",
            "Loss: 0.06419571489095688\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.05399559810757637\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.08226301521062851\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.08540153503417969\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.09317243844270706\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.06740365922451019\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.07938067615032196\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.07686253637075424\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.0866236761212349\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.05567006394267082\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.06694856286048889\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.07491055130958557\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.07772445678710938\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.09702867269515991\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.05466415733098984\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.05358231067657471\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.10474596172571182\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.04989457130432129\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.06830120086669922\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.05786603316664696\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.08811590820550919\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.06609676778316498\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.07789166271686554\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.06699557602405548\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.07566589117050171\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.08588354289531708\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.1149575412273407\n",
            "Accuracy: 0.875\n",
            "Loss: 0.06557439267635345\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.08490386605262756\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.05202408879995346\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.08690334856510162\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.05817083269357681\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.05766948312520981\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.047694917768239975\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.08293727785348892\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.07468611747026443\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.054913055151700974\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.040536068379879\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.0938250720500946\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.07766606658697128\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.045715827494859695\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.07195731997489929\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.09501078724861145\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.04584526643157005\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.08506426960229874\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.05845743045210838\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.06595495343208313\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.04383527487516403\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.05806143209338188\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.0662059634923935\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.09556376934051514\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.057171568274497986\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.07570286095142365\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.05677584558725357\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.11088482290506363\n",
            "Accuracy: 0.875\n",
            "Loss: 0.04480878263711929\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.06294680386781693\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.07158849388360977\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.07337629050016403\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.047853127121925354\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.05259096622467041\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.06469812244176865\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.06603656709194183\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.04552283510565758\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.1108437329530716\n",
            "Accuracy: 0.859375\n",
            "Loss: 0.053983546793460846\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.07535307854413986\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.0462745763361454\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.07253025472164154\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.03160800039768219\n",
            "Accuracy: 1.0\n",
            "Loss: 0.0633191242814064\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.06267936527729034\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.04594246298074722\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.04289419576525688\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.05093531683087349\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.07688219845294952\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.053509753197431564\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.044205762445926666\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.07379083335399628\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.06988739967346191\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.0732068344950676\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.09484872221946716\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.05431123822927475\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.08522213995456696\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.06340164691209793\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.06385727971792221\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.09402120858430862\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.05475855618715286\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.04302883520722389\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.05189760774374008\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.07637940347194672\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.0692020058631897\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.052520886063575745\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.07507847249507904\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.0791855975985527\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.11471040546894073\n",
            "Accuracy: 0.875\n",
            "Loss: 0.04654886946082115\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.07449241727590561\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.06423841416835785\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.04146410524845123\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.06419316679239273\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.07159154862165451\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.05352705717086792\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.05413131043314934\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.0622965469956398\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.051903486251831055\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.05337056890130043\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.10417571663856506\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.09402092546224594\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.09957031905651093\n",
            "Accuracy: 0.875\n",
            "Loss: 0.0641784816980362\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.04515997692942619\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.06374376267194748\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.0910070464015007\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.06375491619110107\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.05615111440420151\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.05174796283245087\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.05146497115492821\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.07554370164871216\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.06929551064968109\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.061801061034202576\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.0717427209019661\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.05105135962367058\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.06400830298662186\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.04299424588680267\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.054162345826625824\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.0722881481051445\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.08469299972057343\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.10352026671171188\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.056511905044317245\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.08332238346338272\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.09252510219812393\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.0601494163274765\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.04226990416646004\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.0946168377995491\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.07309895753860474\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.08528989553451538\n",
            "Accuracy: 0.921875\n",
            "Loss: 0.1011425256729126\n",
            "Accuracy: 0.890625\n",
            "Loss: 0.04629525914788246\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.07263237982988358\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.0527583546936512\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.04153069853782654\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.05112703889608383\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.05492110177874565\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.03652593493461609\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.06148122623562813\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.051434099674224854\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.04998067766427994\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.05229824781417847\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.061489492654800415\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.05147620663046837\n",
            "Accuracy: 0.96875\n",
            "Loss: 0.07079654932022095\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.10799530893564224\n",
            "Accuracy: 0.875\n",
            "Loss: 0.06364753842353821\n",
            "Accuracy: 0.953125\n",
            "Loss: 0.040197890251874924\n",
            "Accuracy: 0.984375\n",
            "Loss: 0.09191469848155975\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.07403919100761414\n",
            "Accuracy: 0.9375\n",
            "Loss: 0.09184486418962479\n",
            "Accuracy: 0.90625\n",
            "Loss: 0.03411759436130524\n",
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inside the training loop\n",
        "for inputs, labels in train_loader:\n",
        "    optimizer.zero_grad()\n",
        "    inputs = inputs.unsqueeze(1)\n",
        "    outputs = model(inputs)\n",
        "    outputs = F.softmax(outputs, dim=1)\n",
        "    labels = labels.squeeze(1)\n",
        "\n",
        "\n",
        "    # Calculating loss\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Calculate accuracy\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct = (predicted == labels).sum().item()\n",
        "    total = labels.size(0)\n",
        "    accuracy = correct / total\n",
        "\n",
        "    # Convert accuracy to percentage\n",
        "    accuracy_percent = accuracy * 100\n",
        "\n",
        "    print(\"Loss:\", loss.item())\n",
        "    print(\"Accuracy:\", accuracy_percent,\"%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84LY5lHXJUrH",
        "outputId": "94aeee91-86a0-4313-a32e-57bdc4624077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0561550036072731\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04472127556800842\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.046054087579250336\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05587613582611084\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.034338779747486115\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.055845778435468674\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04808779060840607\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04501601681113243\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.08283865451812744\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.038430068641901016\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.06712861359119415\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06665927171707153\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.07777377218008041\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.036482080817222595\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.047193851321935654\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05609679967164993\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.06634669750928879\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06670989096164703\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.024073541164398193\n",
            "Accuracy: 100.0 %\n",
            "Loss: 0.056858524680137634\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.034449636936187744\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.05547112971544266\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05553539842367172\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.0554194413125515\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05543297156691551\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.06776994466781616\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06622001528739929\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.056300532072782516\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05039886012673378\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.06677268445491791\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.0452188178896904\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05646970868110657\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04625151306390762\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05549689009785652\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.024796409532427788\n",
            "Accuracy: 100.0 %\n",
            "Loss: 0.04523950070142746\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.07767989486455917\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.05623837560415268\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04537007957696915\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05605558305978775\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.0876920148730278\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.06630894541740417\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06656645983457565\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.03435016795992851\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.03475571051239967\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.08760763704776764\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.05633407458662987\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05630490928888321\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.09023076295852661\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.04576537758111954\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.04570920765399933\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.06637480109930038\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06686856597661972\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.11119955778121948\n",
            "Accuracy: 85.9375 %\n",
            "Loss: 0.06642123311758041\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06695406138896942\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.045370813459157944\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.09924226999282837\n",
            "Accuracy: 89.0625 %\n",
            "Loss: 0.05596986785531044\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.07224766910076141\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.057292018085718155\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04503129795193672\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.07702912390232086\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.04518885910511017\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.04567038267850876\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.055762097239494324\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.06631770730018616\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.045458052307367325\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.08775245398283005\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.03443063795566559\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.06706802546977997\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.04556073620915413\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.06636442244052887\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06677835434675217\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.04619143158197403\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.04577096179127693\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.07675059884786606\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.03469764068722725\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.04511027783155441\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.04509139060974121\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.044963426887989044\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.0979890525341034\n",
            "Accuracy: 89.0625 %\n",
            "Loss: 0.09877005219459534\n",
            "Accuracy: 89.0625 %\n",
            "Loss: 0.04552121087908745\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.045049842447042465\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.03444782644510269\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.07709671556949615\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.06658817082643509\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.055689744651317596\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.0775066390633583\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.0661114901304245\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.0874580517411232\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.06589638441801071\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06617692112922668\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06638898700475693\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.03541544824838638\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.07683038711547852\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.09811870008707047\n",
            "Accuracy: 89.0625 %\n",
            "Loss: 0.055699385702610016\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.1088922768831253\n",
            "Accuracy: 87.5 %\n",
            "Loss: 0.06600571423768997\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.03528577461838722\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.056673042476177216\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.03476586192846298\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.04505128413438797\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.06644939631223679\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.11821421980857849\n",
            "Accuracy: 85.9375 %\n",
            "Loss: 0.05580500513315201\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.034506700932979584\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.07685383409261703\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.06650295853614807\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.023945890367031097\n",
            "Accuracy: 100.0 %\n",
            "Loss: 0.07647015899419785\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.05552171543240547\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04522871971130371\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.04502612352371216\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.07666154205799103\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.05552016943693161\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05574411153793335\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.034448377788066864\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.05548502877354622\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.03514726832509041\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.023885218426585197\n",
            "Accuracy: 100.0 %\n",
            "Loss: 0.0660550594329834\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.0267340075224638\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.055447302758693695\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05561297386884689\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.08751369267702103\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.0452747568488121\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.0759018287062645\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.04281800612807274\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.07920253276824951\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.05557131767272949\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05673373490571976\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.066429004073143\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.03622719645500183\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.05612567067146301\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.07769110798835754\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.06588063389062881\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.08749865740537643\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.06598500162363052\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06629066169261932\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.08723887801170349\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.0453326590359211\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.04477265849709511\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.04574882239103317\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.06795517355203629\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.055346470326185226\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04919726401567459\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.03603070229291916\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.034424297511577606\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.04521412029862404\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.045393384993076324\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.06653021275997162\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.05625293031334877\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.03504280745983124\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.07667393237352371\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.07678042352199554\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.04489731788635254\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.059508491307497025\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06673303246498108\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.055632010102272034\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.08727061748504639\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.06599821150302887\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.08749917149543762\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.0765530988574028\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.04512949660420418\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.06621675193309784\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.07670868933200836\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.05686056241393089\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.06612575054168701\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.065975621342659\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.05583013594150543\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.08718270808458328\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.05573553591966629\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05567706376314163\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04515843093395233\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.0660436600446701\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.08745041489601135\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.0661030262708664\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.04488110914826393\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.0662088394165039\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.03424244374036789\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.07708621770143509\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.05941210687160492\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.03443118929862976\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.0659564808011055\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.055314112454652786\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04525286331772804\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.045057788491249084\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.03423180803656578\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.04530554637312889\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.0661737248301506\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.0556304007768631\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.03418077528476715\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.04470057040452957\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.055142469704151154\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.06732524931430817\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.02385466918349266\n",
            "Accuracy: 100.0 %\n",
            "Loss: 0.055642444640398026\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04475501552224159\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05552729591727257\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04484039172530174\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.055362652987241745\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04521169140934944\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.06593623012304306\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.0764947310090065\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.057313304394483566\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.07704658806324005\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.055266935378313065\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.03426443785429001\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.06620074808597565\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.055390484631061554\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.06685145944356918\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.04480190575122833\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.03485133871436119\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.044953782111406326\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.0555286779999733\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04507223889231682\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.07657737284898758\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.03398755192756653\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.05524197220802307\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.06598950177431107\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06891565024852753\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06600658595561981\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.05552805960178375\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.045272644609212875\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.07820804417133331\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.033923957496881485\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.05544089525938034\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.045110493898391724\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.09885366261005402\n",
            "Accuracy: 89.0625 %\n",
            "Loss: 0.06693103164434433\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.058147963136434555\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06615371257066727\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.03442499041557312\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.055839601904153824\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.06624001264572144\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.04690269008278847\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.06671245396137238\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06625886261463165\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.03414323180913925\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.05597683787345886\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.055365800857543945\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04475909098982811\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.07657971233129501\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.07727416604757309\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.03547050803899765\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.04470255970954895\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.03416777774691582\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.055580999702215195\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.06594303995370865\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.035324349999427795\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.03417070955038071\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.034324418753385544\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.04523459076881409\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05548971891403198\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.06586594134569168\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.055395059287548065\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.058541439473629\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.1083875298500061\n",
            "Accuracy: 87.5 %\n",
            "Loss: 0.07680551707744598\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.02375102788209915\n",
            "Accuracy: 100.0 %\n",
            "Loss: 0.07684212923049927\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.06612327694892883\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.04494459927082062\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.07662424445152283\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.04494310915470123\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.03451327234506607\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.03422936797142029\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.04468516632914543\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.06597886979579926\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.0788678377866745\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.06591580808162689\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.05560008063912392\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04478110000491142\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.055334754288196564\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.06602029502391815\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06679661571979523\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.044777724891901016\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.07663482427597046\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.044903989881277084\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.0672655999660492\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.04456418752670288\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.045698635280132294\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.08736537396907806\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.03467228636145592\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.034458309412002563\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.04486531764268875\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.04501098766922951\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05533087998628616\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05555133894085884\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.0665455013513565\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.07666124403476715\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.03437792509794235\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.06596554815769196\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.05521402135491371\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.044517964124679565\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.055399373173713684\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.076457679271698\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.04343290627002716\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.04503380134701729\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05560273304581642\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.06669486314058304\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.045348092913627625\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.055475685745477676\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05544169992208481\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.03429891914129257\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.06568799167871475\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.055279623717069626\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04626280814409256\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.035251516848802567\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.05570479482412338\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04579438269138336\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.06611819565296173\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.04511938616633415\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.03408150374889374\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.06610260903835297\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.023655954748392105\n",
            "Accuracy: 100.0 %\n",
            "Loss: 0.07665535807609558\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.044646020978689194\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.04479260370135307\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.06603706628084183\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.034958209842443466\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.044620972126722336\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.034355297684669495\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.08717851340770721\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.06593489646911621\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.07739052921533585\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.04483625665307045\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.03394952416419983\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.05574402958154678\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04487420991063118\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.023391522467136383\n",
            "Accuracy: 100.0 %\n",
            "Loss: 0.04482327774167061\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.06593095511198044\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06609218567609787\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.04517590254545212\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.034391239285469055\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.024030989035964012\n",
            "Accuracy: 100.0 %\n",
            "Loss: 0.07654491811990738\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.034570880234241486\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.023590141907334328\n",
            "Accuracy: 100.0 %\n",
            "Loss: 0.0566653348505497\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.08730949461460114\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.05533161014318466\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05559953302145004\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04473278298974037\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05546678602695465\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.02358512207865715\n",
            "Accuracy: 100.0 %\n",
            "Loss: 0.05515473708510399\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.02330322191119194\n",
            "Accuracy: 100.0 %\n",
            "Loss: 0.04438639432191849\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.055341850966215134\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.03407220542430878\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.04454239457845688\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.055385805666446686\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.03417065367102623\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.03417626768350601\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.0553874671459198\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05647007375955582\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.07641691714525223\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.04489630460739136\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.055271316319704056\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.06595885753631592\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.07676893472671509\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.03409475088119507\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.03424370288848877\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.07667449116706848\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.0449335053563118\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.07685089111328125\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.07638281583786011\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.05529381334781647\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04456941783428192\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.045032091438770294\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.03418109193444252\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.05730292946100235\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.044807326048612595\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05528368055820465\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.055631935596466064\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.07679714262485504\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.044854842126369476\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05605291575193405\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.034220535308122635\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.04462692514061928\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.044512756168842316\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05549531430006027\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05544666573405266\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.02365444414317608\n",
            "Accuracy: 100.0 %\n",
            "Loss: 0.06572616845369339\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.07646284997463226\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.0657387226819992\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06620942801237106\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06597693264484406\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.044481925666332245\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.06633000075817108\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.05510726571083069\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.06587249785661697\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.03412694111466408\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.06658147275447845\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.07632996886968613\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.055174630135297775\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.07132699340581894\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.055209673941135406\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.06571923196315765\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.034352995455265045\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.0448458157479763\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05515434220433235\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05528644472360611\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.034295424818992615\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.05541218817234039\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04462239146232605\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.07635465264320374\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.0341874435544014\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.07651668787002563\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.0661570206284523\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06571456789970398\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.05515395477414131\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.043581441044807434\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.0665319412946701\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.044574737548828125\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.06570667773485184\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06576773524284363\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.08702044934034348\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.06604106724262238\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06581868976354599\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.05505410209298134\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.055080346763134\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.06588789820671082\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.055240198969841\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05519486591219902\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.0450909286737442\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.07668480277061462\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.06580330431461334\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.04456831142306328\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.044529568403959274\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.03690120950341225\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.055234432220458984\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.044838398694992065\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.056759171187877655\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.06596876680850983\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.03426042199134827\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.07651950418949127\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.05544732138514519\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.09803351759910583\n",
            "Accuracy: 89.0625 %\n",
            "Loss: 0.07458576560020447\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.0659913569688797\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.07641618698835373\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.08695358783006668\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.055345069617033005\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.023931637406349182\n",
            "Accuracy: 100.0 %\n",
            "Loss: 0.04440966993570328\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.04474475607275963\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05656597018241882\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.03404860571026802\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.060383547097444534\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.05516653507947922\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.07735082507133484\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.06617844104766846\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.08759691566228867\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.05527796223759651\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.07680514454841614\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.04497908428311348\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.044704094529151917\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.06596016883850098\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.07643798738718033\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.04502909630537033\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.045608535408973694\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.04474565386772156\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.06649762392044067\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.0764349028468132\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.09763879328966141\n",
            "Accuracy: 89.0625 %\n",
            "Loss: 0.055275142192840576\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05639377981424332\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05525058135390282\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.09765768051147461\n",
            "Accuracy: 89.0625 %\n",
            "Loss: 0.035096071660518646\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.09762699902057648\n",
            "Accuracy: 89.0625 %\n",
            "Loss: 0.023723909631371498\n",
            "Accuracy: 100.0 %\n",
            "Loss: 0.06594589352607727\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.04467277228832245\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.023518379777669907\n",
            "Accuracy: 100.0 %\n",
            "Loss: 0.0341358408331871\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.07643008232116699\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.07638444751501083\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.055220890790224075\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.03397667407989502\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.04452485591173172\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05520157516002655\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.09752320498228073\n",
            "Accuracy: 89.0625 %\n",
            "Loss: 0.04462915286421776\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.033998772501945496\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.055187471210956573\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04446088522672653\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.023302916437387466\n",
            "Accuracy: 100.0 %\n",
            "Loss: 0.06657920032739639\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06572271883487701\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.05522334948182106\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.03412490338087082\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.07636339217424393\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.0549936518073082\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.055111151188611984\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.034289732575416565\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.03407975658774376\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.0447886623442173\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.024822155013680458\n",
            "Accuracy: 100.0 %\n",
            "Loss: 0.044554099440574646\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05547894537448883\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.055332206189632416\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04451463744044304\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.03411105275154114\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.023515932261943817\n",
            "Accuracy: 100.0 %\n",
            "Loss: 0.08719699084758759\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.05652839317917824\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.08727633208036423\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.0551617294549942\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.02365373633801937\n",
            "Accuracy: 100.0 %\n",
            "Loss: 0.03399105742573738\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.06666477024555206\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.05529411509633064\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04515684023499489\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.033877529203891754\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.06580419838428497\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.05524659529328346\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05559926852583885\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05532058700919151\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05586622655391693\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.08801829814910889\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.08699074387550354\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.04734111577272415\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.045028697699308395\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.08694648742675781\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.05541474372148514\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.076392762362957\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.087184377014637\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.0554196760058403\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.0658927783370018\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06587968021631241\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.04463070258498192\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05500577762722969\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05512751266360283\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.07641813158988953\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.055038969963788986\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.055879928171634674\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05505546182394028\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.07623971998691559\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.06601088494062424\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.033941805362701416\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.08688602596521378\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.04463167488574982\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.06580521166324615\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06582856923341751\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.04444611445069313\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.04454045742750168\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.03458536043763161\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.05601460859179497\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.0869433581829071\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.03393460437655449\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.04457055777311325\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.055405016988515854\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05558003485202789\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05532802641391754\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.044520217925310135\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.055121537297964096\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04458589479327202\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.044620856642723083\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.03399604186415672\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.07641018182039261\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.04451694339513779\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.08308413624763489\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.03215140104293823\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.044590361416339874\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.06619946658611298\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.07676606625318527\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.044651709496974945\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.03401608392596245\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.06579655408859253\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.09911534935235977\n",
            "Accuracy: 89.0625 %\n",
            "Loss: 0.03413384407758713\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.04467376321554184\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.08765657991170883\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.03421512246131897\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.055024415254592896\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04470483958721161\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05509364604949951\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.06595411896705627\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.045738328248262405\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.033843234181404114\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.033820100128650665\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.055198609828948975\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.03400702401995659\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.06589525938034058\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.023293614387512207\n",
            "Accuracy: 100.0 %\n",
            "Loss: 0.055039554834365845\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04432985931634903\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.06569508463144302\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06576341390609741\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06570162624120712\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.055274598300457\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05606268346309662\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.0871908962726593\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.07641538232564926\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.055140309035778046\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.023395560681819916\n",
            "Accuracy: 100.0 %\n",
            "Loss: 0.023322954773902893\n",
            "Accuracy: 100.0 %\n",
            "Loss: 0.07634307444095612\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.06597283482551575\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.04453933984041214\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.03388843312859535\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.0662909597158432\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.04524776339530945\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.03391773998737335\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.08681879937648773\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.06599559634923935\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.034295231103897095\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.0550515279173851\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04038042947649956\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.07673428952693939\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.0869118869304657\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.04493836686015129\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.044904857873916626\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.044734060764312744\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.03448236733675003\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.0447004996240139\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.06589339673519135\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.07663088291883469\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.06595384329557419\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06576913595199585\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.03377877548336983\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.07723437249660492\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.07739686965942383\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.05524265766143799\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.044656768441200256\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05521463602781296\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05507739633321762\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.044438041746616364\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.033818792551755905\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.07618957757949829\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.04456169158220291\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05506858229637146\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.076142817735672\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.055047813802957535\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.033840544521808624\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.056408800184726715\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05516617000102997\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05521943047642708\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05592150613665581\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.06553714722394943\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.05512777715921402\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.044393278658390045\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05508922040462494\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05500005558133125\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.055483926087617874\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.06408728659152985\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06586288660764694\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.044740043580532074\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.08673275262117386\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.03394485265016556\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.0655602514743805\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.055057600140571594\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04452524334192276\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.03379718214273453\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.07659925520420074\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.1097688302397728\n",
            "Accuracy: 87.5 %\n",
            "Loss: 0.03394163399934769\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.05501982569694519\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.03383616730570793\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.05516434833407402\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05491308122873306\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.055095892399549484\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05501526966691017\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.07620633393526077\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.07633467763662338\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.06571156531572342\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.07619957625865936\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.06623880565166473\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.045601919293403625\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.08692881464958191\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.04451441019773483\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.06557159125804901\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.04890692979097366\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05502736568450928\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05506020784378052\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04447261244058609\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05575760826468468\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05576247721910477\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04496428370475769\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.055246345698833466\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.0868084505200386\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.04449862241744995\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05529619753360748\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.044517289847135544\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.055006202310323715\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.11861367523670197\n",
            "Accuracy: 85.9375 %\n",
            "Loss: 0.05494435504078865\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.03384337201714516\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.06556963175535202\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06575428694486618\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.09749735891819\n",
            "Accuracy: 89.0625 %\n",
            "Loss: 0.07622886449098587\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.033785153180360794\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.04436555132269859\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.03427740931510925\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.05513269454240799\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05496161803603172\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05491194874048233\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05533243715763092\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.06628002971410751\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.035280920565128326\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.054935019463300705\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.06578109413385391\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.05510726198554039\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.0445779450237751\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.04444161802530289\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.055138688534498215\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.07618936896324158\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.08672234416007996\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.044419318437576294\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.03388037905097008\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.023542065173387527\n",
            "Accuracy: 100.0 %\n",
            "Loss: 0.03376952186226845\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.044398412108421326\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.044305138289928436\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.07619920372962952\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.05505592003464699\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.03429291769862175\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.05536424368619919\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.055138785392045975\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.0341431200504303\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.054949648678302765\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.06602033227682114\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06559138745069504\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.04429572448134422\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.06561897695064545\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.04499567300081253\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.03434441611170769\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.033521465957164764\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.05518272891640663\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.07755664736032486\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.06582964956760406\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.05528461933135986\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.06583423167467117\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.0656876340508461\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.055158235132694244\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.057043422013521194\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04455491527915001\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.04466445744037628\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.023980116471648216\n",
            "Accuracy: 100.0 %\n",
            "Loss: 0.044444482773542404\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.04430243745446205\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.06557613611221313\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.0559927299618721\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.044299107044935226\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.02325553633272648\n",
            "Accuracy: 100.0 %\n",
            "Loss: 0.04443245381116867\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.06618653237819672\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.07641705870628357\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.03395061194896698\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.06588906049728394\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.05495980381965637\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04453957825899124\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.06550788134336472\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.04435254633426666\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.044324785470962524\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.06578045338392258\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.07622652500867844\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.07636650651693344\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.05595381557941437\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.044417254626750946\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.023176027461886406\n",
            "Accuracy: 100.0 %\n",
            "Loss: 0.05506584420800209\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.07614544779062271\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.033709507435560226\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.08684427291154861\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.055125392973423004\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.0763741135597229\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.06576205044984818\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.055000778287649155\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04440637677907944\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05512865260243416\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.08702093362808228\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.044340234249830246\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05501698702573776\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.03369944915175438\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.05500058829784393\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.033785201609134674\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.07697420567274094\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.07606697082519531\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.06547247618436813\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.023240111768245697\n",
            "Accuracy: 100.0 %\n",
            "Loss: 0.04445388540625572\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.04448288679122925\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.06551884859800339\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.09735958278179169\n",
            "Accuracy: 89.0625 %\n",
            "Loss: 0.04426818713545799\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05495670065283775\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04438169300556183\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.0662132054567337\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.04443943127989769\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.06653257459402084\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06564576923847198\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06593634933233261\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06561125069856644\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.04435533285140991\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05511854961514473\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.03413216397166252\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.055072490125894547\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.07622293382883072\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.07681921869516373\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.03448590636253357\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.05496535450220108\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.07611192762851715\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.04436206817626953\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05492998659610748\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.055227674543857574\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.06560453772544861\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06233669072389603\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06560643017292023\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.023429343476891518\n",
            "Accuracy: 100.0 %\n",
            "Loss: 0.044929757714271545\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.044964417815208435\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.034102704375982285\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.055102068930864334\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04439787566661835\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.02344147488474846\n",
            "Accuracy: 100.0 %\n",
            "Loss: 0.055086247622966766\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.0338001623749733\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.04441305249929428\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.0551803782582283\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.02322632446885109\n",
            "Accuracy: 100.0 %\n",
            "Loss: 0.04448005184531212\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.03378498926758766\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.054528456181287766\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.044886842370033264\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.07612156867980957\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.05488414317369461\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.0445595309138298\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05504390969872475\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05498667433857918\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.03381277620792389\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.03439093008637428\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.055005818605422974\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.06551659852266312\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.05505143851041794\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05496016517281532\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04425773397088051\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05499936267733574\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.06560955196619034\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.07613024860620499\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.07617347687482834\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.06665925681591034\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.044383734464645386\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.04461875557899475\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05503056198358536\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04439616575837135\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.033798519521951675\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.044529303908348083\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.044406428933143616\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.06563308089971542\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06557033210992813\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.04426004737615585\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.03379175812005997\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.06592629849910736\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.07643856108188629\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.04438374936580658\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.07619316130876541\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.05496396869421005\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.054979510605335236\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04428061097860336\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.055076852440834045\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05490434169769287\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.07645370811223984\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.04431876540184021\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.033639486879110336\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.06531288474798203\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.05488414317369461\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.033727485686540604\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.07705595344305038\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.054965078830718994\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05486670136451721\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.08671166747808456\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.07613879442214966\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.055003926157951355\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.08670303225517273\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.0762173980474472\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.07603113353252411\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.06553862988948822\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.07620617747306824\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.05485490709543228\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.054862719029188156\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04427361860871315\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.07487759739160538\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.05486754700541496\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04443114995956421\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.08720221370458603\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.04428519681096077\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.09734632819890976\n",
            "Accuracy: 89.0625 %\n",
            "Loss: 0.055060967803001404\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.06559160351753235\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.04425977170467377\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.0867539495229721\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.0655311644077301\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.05502728372812271\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.02304339036345482\n",
            "Accuracy: 100.0 %\n",
            "Loss: 0.05497901514172554\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.07617609947919846\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.0655055120587349\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06552021950483322\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06659266352653503\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.04429576173424721\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.0549563504755497\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05856579542160034\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.03397330641746521\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.03417103737592697\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.07624033093452454\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.03364895284175873\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.06559357047080994\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.08676701784133911\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.02323792688548565\n",
            "Accuracy: 100.0 %\n",
            "Loss: 0.0340271033346653\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.07622431963682175\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.07627078145742416\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.06542478501796722\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.044302914291620255\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.06563705950975418\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.07604631036520004\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.044791899621486664\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.044414982199668884\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.0655081570148468\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.1071201041340828\n",
            "Accuracy: 87.5 %\n",
            "Loss: 0.054950933903455734\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04425765573978424\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.03380107879638672\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.04646053537726402\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.03373867645859718\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.0654260590672493\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.03379456326365471\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.05493139103055\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.0762253925204277\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.06583577394485474\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.03360007703304291\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.07611656188964844\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.04512203484773636\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.06674475967884064\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.044278405606746674\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.055727798491716385\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.03383488580584526\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.0655471459031105\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.03891601040959358\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.08687477558851242\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.08775578439235687\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.06676387041807175\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.055252403020858765\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.044952526688575745\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.055171240121126175\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.06570634245872498\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.06568481773138046\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.03363290801644325\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.03361603245139122\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.044299203902482986\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.054924823343753815\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04492736980319023\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.04421931877732277\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.044273488223552704\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.08669345080852509\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.04443827271461487\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.09718416631221771\n",
            "Accuracy: 89.0625 %\n",
            "Loss: 0.044666096568107605\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.0443413183093071\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.0442790761590004\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05491499975323677\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.07607991993427277\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.033799219876527786\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.05519719049334526\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04433999955654144\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.07606848329305649\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.05494644492864609\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.06558254361152649\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.05493079125881195\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04782845079898834\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.09829217940568924\n",
            "Accuracy: 89.0625 %\n",
            "Loss: 0.06559398770332336\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.03371165320277214\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.023214122280478477\n",
            "Accuracy: 100.0 %\n",
            "Loss: 0.055192846804857254\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.07617497444152832\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.04444735497236252\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.044334422796964645\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.054945193231105804\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.07619072496891022\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.05514445900917053\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05531194061040878\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.054890576750040054\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.06543292105197906\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.07608338445425034\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.07604043185710907\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.04477369040250778\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.04428814351558685\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.04429337754845619\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.06551870703697205\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.054846327751874924\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05491381883621216\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.0868522971868515\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.056331098079681396\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.03357364237308502\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.06549064069986343\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.054868318140506744\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.03371407091617584\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.04461536929011345\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05480682849884033\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.04709828644990921\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.033664505928754807\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.044964056462049484\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05526997148990631\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.05512189492583275\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.07620909810066223\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.07674737274646759\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.044341713190078735\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.07027193158864975\n",
            "Accuracy: 92.1875 %\n",
            "Loss: 0.09725787490606308\n",
            "Accuracy: 89.0625 %\n",
            "Loss: 0.04453032836318016\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.03389773517847061\n",
            "Accuracy: 98.4375 %\n",
            "Loss: 0.04430754855275154\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.04454275965690613\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.06563536822795868\n",
            "Accuracy: 93.75 %\n",
            "Loss: 0.08682221919298172\n",
            "Accuracy: 90.625 %\n",
            "Loss: 0.04423297196626663\n",
            "Accuracy: 96.875 %\n",
            "Loss: 0.05489647388458252\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.054946430027484894\n",
            "Accuracy: 95.3125 %\n",
            "Loss: 0.022946996614336967\n",
            "Accuracy: 100.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('hi')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGjr0n55J58C",
        "outputId": "a29c45bd-87a9-4081-b0a6-b9b4e6734c85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-jVIYnOELkIB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}